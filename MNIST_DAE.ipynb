{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 0.0471\n",
      "Epoch 2, train loss: 0.0378\n",
      "Epoch 3, train loss: 0.0353\n",
      "Epoch 4, train loss: 0.0325\n",
      "Epoch 5, train loss: 0.0306\n",
      "Epoch 6, train loss: 0.0294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m latent_dir \u001b[39m=\u001b[39m train_store_latent(\u001b[39m'\u001b[39;49m\u001b[39mDAE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m30\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m0.001\u001b[39;49m, \u001b[39m42\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/hpcstor6/scratch01/f/fatemeh.afrasiabi001/MNIST_VAE/train.py:59\u001b[0m, in \u001b[0;36mtrain_store_latent\u001b[0;34m(AE_type, epochs, latent_len, batch_size, learning_rate, random_seed, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Train and fit the model\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m fit_model(model, train_data, epochs, learning_rate, device,\n\u001b[1;32m     60\u001b[0m         criterion, model_dir, batch_size, verbose,\n\u001b[1;32m     61\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, add_noise\u001b[39m=\u001b[39;49madd_noise, validation\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m \u001b[39m# model.load_state_dict(torch.load(f'{model_dir}/{digit}_{latent_len}_{random_seed}.pth'))\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[39m# Store train and test latent spaces\u001b[39;00m\n\u001b[1;32m     65\u001b[0m train_loss, train_latent \u001b[39m=\u001b[39m test_model(model, train_data, device, criterion, add_noise)\n",
      "File \u001b[0;32m/hpcstor6/scratch01/f/fatemeh.afrasiabi001/MNIST_VAE/util.py:156\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model, train_dataset, num_epochs, lr, device, criterion, model_dir, verbose, batch_size, num_workers, add_noise, validation)\u001b[0m\n\u001b[1;32m    154\u001b[0m val_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 156\u001b[0m     train_epoch_loss \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m    157\u001b[0m         model, train_loader, optimizer, device, criterion, add_noise)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m validation:\n\u001b[1;32m    159\u001b[0m         val_epoch_loss, _ \u001b[39m=\u001b[39m test_model(model, val_loader, device, criterion, add_noise)\n",
      "File \u001b[0;32m/hpcstor6/scratch01/f/fatemeh.afrasiabi001/MNIST_VAE/util.py:186\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, device, criterion, add_noise)\u001b[0m\n\u001b[1;32m    184\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    185\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 186\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    187\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    188\u001b[0m train_loss \u001b[39m=\u001b[39m running_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/.conda/envs/tda-py/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/tda-py/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/tda-py/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/tda-py/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/.conda/envs/tda-py/lib/python3.9/site-packages/torch/optim/adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 307\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from train import *\n",
    "from util import *\n",
    "latent_dir = train_store_latent('DAE', 30, 32, 64, 0.001, 42, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "latent_df = pd.DataFrame()\n",
    "for digit in range(10):\n",
    "    df = pd.read_csv(f'AECompare/MNIST_digits_latents/AE_latents/{digit}_30_0_train.csv',\n",
    "     names=[a for a in range(0,30)])\n",
    "    df['target'] = int(digit)\n",
    "    latent_df = pd.concat([latent_df, df])\n",
    "    \n",
    "# Shuffle the new MNIST data    \n",
    "latent_df = latent_df.sample(frac=1).reset_index(drop=True)\n",
    "x, y = latent_df[[a for a in range(0,30)]].to_numpy(), latent_df['target'].to_numpy()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036553</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.066184</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.013772</td>\n",
       "      <td>0.110793</td>\n",
       "      <td>0.043746</td>\n",
       "      <td>0.050532</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23597</th>\n",
       "      <td>0.013118</td>\n",
       "      <td>0.087671</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.056542</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.303403</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.008328</td>\n",
       "      <td>0.090194</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.009361</td>\n",
       "      <td>0.072611</td>\n",
       "      <td>0.106439</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23605</th>\n",
       "      <td>0.037457</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.064146</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.097986</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>0.050967</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017515</td>\n",
       "      <td>0.024970</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.058448</td>\n",
       "      <td>0.090370</td>\n",
       "      <td>0.060696</td>\n",
       "      <td>0.009710</td>\n",
       "      <td>0.012681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23609</th>\n",
       "      <td>0.076808</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.035123</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>0.431155</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.010163</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23611</th>\n",
       "      <td>0.032493</td>\n",
       "      <td>0.048621</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.069437</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.025873</td>\n",
       "      <td>0.062026</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.035295</td>\n",
       "      <td>0.133214</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.026796</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49441</th>\n",
       "      <td>0.005155</td>\n",
       "      <td>0.154809</td>\n",
       "      <td>0.116804</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.005795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.126177</td>\n",
       "      <td>0.123393</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.008106</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3817</th>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.104487</td>\n",
       "      <td>0.241295</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.182964</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41066</th>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.083761</td>\n",
       "      <td>0.203158</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.028113</td>\n",
       "      <td>0.076942</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13155</th>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.067841</td>\n",
       "      <td>0.209516</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.021806</td>\n",
       "      <td>0.142110</td>\n",
       "      <td>0.005826</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45990</th>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.062838</td>\n",
       "      <td>0.170594</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.192892</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.007289</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.036553  0.036253  0.003741  0.066184  0.005580  0.075897  0.003320   \n",
       "23597  0.013118  0.087671  0.001992  0.056542  0.002755  0.303403  0.001395   \n",
       "23605  0.037457  0.045796  0.010760  0.064146  0.012196  0.097986  0.009345   \n",
       "23609  0.076808  0.002586  0.000437  0.010482  0.001285  0.035123  0.000287   \n",
       "23611  0.032493  0.048621  0.002040  0.069437  0.004193  0.014118  0.001770   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "49441  0.005155  0.154809  0.116804  0.005226  0.005266  0.005184  0.005225   \n",
       "3817   0.003348  0.104487  0.241295  0.003336  0.003341  0.003325  0.003289   \n",
       "41066  0.006627  0.083761  0.203158  0.006729  0.006713  0.006640  0.006524   \n",
       "13155  0.005927  0.067841  0.209516  0.005915  0.005916  0.005910  0.005789   \n",
       "45990  0.002685  0.062838  0.170594  0.002673  0.002709  0.002668  0.002579   \n",
       "\n",
       "              7         8         9  ...        21        22        23  \\\n",
       "0      0.010957  0.033672  0.003816  ...  0.009901  0.010883  0.005476   \n",
       "23597  0.008328  0.090194  0.002002  ...  0.007044  0.006399  0.002321   \n",
       "23605  0.016851  0.050967  0.010555  ...  0.017515  0.024970  0.012127   \n",
       "23609  0.005454  0.014467  0.000423  ...  0.003878  0.001467  0.000964   \n",
       "23611  0.025873  0.062026  0.002279  ...  0.017014  0.002865  0.003488   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "49441  0.005204  0.005498  0.005795  ...  0.005209  0.006610  0.005184   \n",
       "3817   0.003331  0.003334  0.003230  ...  0.003311  0.003680  0.003294   \n",
       "41066  0.006646  0.006722  0.006847  ...  0.006684  0.008795  0.006567   \n",
       "13155  0.005906  0.005805  0.005624  ...  0.005892  0.006547  0.005833   \n",
       "45990  0.002681  0.002593  0.002489  ...  0.002662  0.003162  0.002614   \n",
       "\n",
       "             24        25        26        27        28        29  target  \n",
       "0      0.013772  0.110793  0.043746  0.050532  0.003452  0.004629       0  \n",
       "23597  0.009361  0.072611  0.106439  0.012459  0.001624  0.002783       0  \n",
       "23605  0.020150  0.058448  0.090370  0.060696  0.009710  0.012681       0  \n",
       "23609  0.009738  0.431155  0.004702  0.010163  0.000338  0.000514       0  \n",
       "23611  0.035295  0.133214  0.005265  0.026796  0.001883  0.002859       0  \n",
       "...         ...       ...       ...       ...       ...       ...     ...  \n",
       "49441  0.126177  0.123393  0.005261  0.008106  0.005209  0.005555       9  \n",
       "3817   0.028601  0.182964  0.003292  0.006856  0.003330  0.003464       9  \n",
       "41066  0.028113  0.076942  0.006659  0.013313  0.006623  0.007243       9  \n",
       "13155  0.021806  0.142110  0.005826  0.011402  0.005884  0.006117       9  \n",
       "45990  0.010969  0.192892  0.002603  0.007289  0.002654  0.002856       9  \n",
       "\n",
       "[60000 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderedDF = latent_df.sort_values(\n",
    "  by='target', \n",
    "  ascending=True)\n",
    "orderedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923 0 9 1523\n",
      "6742 1 0 3295\n",
      "5958 2 7 95\n",
      "6131 3 1 703\n",
      "5842 4 8 750\n",
      "5421 5 4 1418\n",
      "5918 6 2 99\n",
      "6265 7 4 1797\n",
      "5851 8 3 383\n",
      "5949 9 6 751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=1111).fit(x)\n",
    "for digit in range(10):\n",
    "    indices = latent_df.index[latent_df.target == digit].tolist()\n",
    "    lst = [kmeans.labels_[i] for i in indices]\n",
    "    mode = max(set(lst), key=lst.count)\n",
    "    #print(lst)\n",
    "    count = 0\n",
    "    for i in lst:\n",
    "        if i != mode:\n",
    "            count += 1\n",
    "    print(len(lst), digit, mode, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "tsne_results = tsne.fit_transform(orderedDF.drop(['target'],axis=1))\n",
    "fig = px.scatter(tsne_results, x=0, y=1,\n",
    "                 color=orderedDF.target.astype(str),\n",
    "                 labels={'0': 'tSNE component 1', '1': 'tSNE component 2'}, color_discrete_sequence=px.colors.qualitative.Plotly)\n",
    "                 #category_order={\"target\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\", \"6\", \"7\",\"8\", \"9\"]})\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,)\n",
    "fig.update_traces(marker=dict(size=4,\n",
    "                              line=dict(width=0.5,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "fig.update_traces(marker=dict(size=4,\n",
    "                              line=dict(width=0.3,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.update_layout({ 'plot_bgcolor': 'rgba(256, 256, 256, 1)', 'paper_bgcolor': 'rgba(256, 256, 256, 1)', })\n",
    "fig.update_layout(legend_traceorder=\"reversed\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/fatemeh.afrasiabi001/.conda/envs/tda-py/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning:\n",
    "\n",
    "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
    "\n",
    "/home/fatemeh.afrasiabi001/.conda/envs/tda-py/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning:\n",
    "\n",
    "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_results = tsne.fit_transform(latent_df.drop(['target'],axis=1))\n",
    "px.scatter(tsne_results, x=0, y=1,\n",
    "                 color=latent_df.target.astype(str),\n",
    "                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tsne_results, x=0, y=1,\n",
    "                 color=latent_df.target.astype(str),\n",
    "                 labels={'0': 'tsne-1', '1': 'tsne-2'}, opacity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Read Chimera jobs results ########\n",
    "import pandas as pd\n",
    "import os\n",
    "directory = 'chimera_jobs/'\n",
    "cols = ['model', 'latent_len', 'random_seed', 'avg_accuracy']\n",
    "AE_df = pd.DataFrame(columns=cols)\n",
    "VAE_df = pd.DataFrame(columns=cols)\n",
    "for f in os.listdir(directory):\n",
    "    if f.endswith(\".out\"): \n",
    "        file = open(directory + f, 'r')\n",
    "        lines = file.readlines()\n",
    "        if lines[1].startswith('VAE'):\n",
    "            for i, line in enumerate(lines):\n",
    "                if i==0 or line.startswith('end'):\n",
    "                    continue\n",
    "                VAE_df.loc[len(VAE_df)] = line.split()\n",
    "        elif lines[1].startswith('AE'):\n",
    "            for i, line in enumerate(lines):\n",
    "                if i==0 or line.startswith('end'):\n",
    "                    continue\n",
    "                AE_df.loc[len(AE_df)] = line.split()\n",
    "        else:\n",
    "            continue\n",
    "sum_vae = VAE_df[['latent_len', 'avg_accuracy']].astype(float).groupby(VAE_df.latent_len).mean()\n",
    "sum_ae = AE_df[['latent_len', 'avg_accuracy']].astype(float).groupby(AE_df.latent_len).mean()\n",
    "sum_vae.to_csv(directory + 'summarized_results/vae_results.csv', index=False)\n",
    "sum_ae.to_csv(directory + 'summarized_results/ae_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent_len</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">35</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">50</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>30</th>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>0.0010</th>\n",
       "      <th>50</th>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 avg_accuracy\n",
       "latent_len learning_rate epochs              \n",
       "65         0.0001        50            0.1750\n",
       "55         0.0001        50            0.1800\n",
       "60         0.0001        50            0.1850\n",
       "50         0.0001        50            0.2000\n",
       "35         0.0001        50            0.2025\n",
       "40         0.0001        50            0.2025\n",
       "45         0.0001        50            0.2050\n",
       "30         0.0001        50            0.2075\n",
       "60         0.0010        50            0.2200\n",
       "30         0.0010        50            0.2300\n",
       "40         0.0010        30            0.2300\n",
       "60         0.0010        30            0.2300\n",
       "65         0.0010        50            0.2400\n",
       "45         0.0010        30            0.2400\n",
       "30         0.0010        30            0.2400\n",
       "65         0.0010        30            0.2400\n",
       "40         0.0010        50            0.2450\n",
       "35         0.0010        50            0.2450\n",
       "                         30            0.2450\n",
       "50         0.0010        30            0.2500\n",
       "                         50            0.2500\n",
       "55         0.0010        30            0.2500\n",
       "45         0.0010        50            0.2500\n",
       "55         0.0010        50            0.2600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('AECompare/chimera_jobs/summarized_results/new_vae_results.csv')\n",
    "df.groupby([df.latent_len, df.learning_rate, df.epochs]).mean().drop('random_seed', axis=1).sort_values(by=['avg_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tda-py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4776f507467e22cdd10079b99b257c5b6d680a1eacdac7182adfe6707076b9ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
